{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract features using Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install gensim==3.6.0 scipy==1.6.3 numpy==1.20.2\n",
    "#! pip install pandas==1.2.4 nltk==3.6.2 matplotlib==3.4.2 scikit-learn==0.24.2 fastdtw==0.3.2 networkx==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/xiaopengxu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import argparse\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inoutpath = '/home/xiaopengxu/Desktop/data-covid-review/2021-05-11/'\n",
    "\n",
    "compdata_path = inoutpath + 'compdata_ext_ref.csv'\n",
    "model_path = inoutpath + 'model.doc2vec'\n",
    "feature_path = inoutpath + 'features.ori_doc2vec.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading & exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(compdata_path):\n",
    "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f') + \": Loading data ...\")\n",
    "    papers = pd.read_csv(compdata_path, index_col=False)\n",
    "    papers.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    print(\"Count number of published papers in archives: \")\n",
    "    print(pd.notnull(papers.published).value_counts())\n",
    "\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-14 13:42:24.140651: Loading data ...\n",
      "Count number of published papers in archives: \n",
      "False    14313\n",
      "True       779\n",
      "Name: published, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "papers = load_data(compdata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(papers):\n",
    "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f') + \": Start preprocessing abstracts ...\")\n",
    "    doc_words = papers['abstract'].map(lambda x: re.sub('[,\\:\\.!?]', ' ', x))  # use only abstracts\n",
    "\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    doc_tokenizers = doc_words.apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in doc_tokenizers]\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-14 13:42:24.644210: Start preprocessing abstracts ...\n"
     ]
    }
   ],
   "source": [
    "docs = pre_process(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(docs):\n",
    "    for i in range(len(docs)):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(docs[i], [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_path: str, docs: list, em_size=50, min_count=2, epochs=40):\n",
    "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f') + \": Start training Doc2Vec model ...\")\n",
    "\n",
    "    corpus = list(process_doc(docs))\n",
    "\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=em_size, min_count=min_count, epochs=epochs)\n",
    "    model.build_vocab(corpus)\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model_path, docs, em_size=50):\n",
    "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f') + \": Get doc2vec embeddings ...\")\n",
    "\n",
    "    model = Doc2Vec.load(model_path)\n",
    "    doc2vec_features = list(map(lambda doc: model.infer_vector(doc), docs))\n",
    "\n",
    "    columns = ['dv ' + str(i + 1) for i in range(em_size)]\n",
    "    pd_doc2vec_features = pd.DataFrame(doc2vec_features, columns=columns)\n",
    "    return pd_doc2vec_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-14 13:42:36.710772: Start training Doc2Vec model ...\n",
      "2021-05-14 13:44:52.935062: Get doc2vec embeddings ...\n"
     ]
    }
   ],
   "source": [
    "train(model_path, docs)\n",
    "pd_doc2vec_features = get_embeddings(model_path, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(filepath, papers, pd_doc2vec_features):\n",
    "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f') + \": Save features ...\")\n",
    "\n",
    "    papers.reset_index(drop=True, inplace=True)\n",
    "    pd_doc2vec_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    pd.concat([papers, pd_doc2vec_features], axis=1).to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-14 13:47:39.575263: Save features ...\n"
     ]
    }
   ],
   "source": [
    "save_features(feature_path, papers, pd_doc2vec_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
