{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo apt-get install -y xvfb ffmpeg\n",
    "#!pip install 'gym==0.10.11'\n",
    "#!pip install 'imageio==2.4.0'\n",
    "#!pip install PILLOW\n",
    "#!pip install 'pyglet==1.3.2'\n",
    "#!pip install pyvirtualdisplay\n",
    "#!pip install tf-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python environments have a `step(action) -> next_time_step` method that applies an action to the environment, and returns the following information about the next step:\n",
    "1. `observation`: This is the part of the environment state that the agent can observe to choose its actions at the next step.\n",
    "2. `reward`: The agent is learning to maximize the sum of these rewards across multiple steps.\n",
    "3. `step_type`: Interactions with the environment are usually part of a sequence/episode. e.g. multiple moves in a game of chess. step_type can be either `FIRST`, `MID` or `LAST` to indicate whether this time step is the first, intermediate or last step in a sequence.\n",
    "4. `discount`: This is a float representing how much to weight the reward at the next time step relative to the reward at the current time step.\n",
    "\n",
    "These are grouped into a named tuple `TimeStep(step_type, reward, discount, observation)`.\n",
    "\n",
    "The interface that all python environments must implement is in `environments/py_environment.PyEnvironment`. The main methods are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Simple Environment for Covid Papers\n",
    "\n",
    "Description:\n",
    "- The academia is accecpting covid-19 papers in order to maximize the total citations. Each paper has an expected number of citations.\n",
    "\n",
    "An environment that represents the game could look like this:\n",
    "1. Actions: We have N actions, that is, accepting 1 paper from all N papers.\n",
    "2. Observations: The papers not published.\n",
    "3. Reward: The objective is maximize the total citations, so we can achieve this using the following reward at the end of the round:\n",
    "  Number of citations of the paper if the paper is not published, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.specs import BoundedArraySpec\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "from tf_agents.trajectories.time_step import TimeStep\n",
    "\n",
    "\n",
    "class CovidPaperSimpleEnv(py_environment.PyEnvironment):\n",
    "    \n",
    "    def __init__(self, discount=1.0):\n",
    "        \"\"\"Initializes CovidPaperSimpleEnv.\n",
    "\n",
    "            Args:\n",
    "                discount: Discount for reward.\n",
    "        \"\"\"\n",
    "        print(\"Init environment ...\")\n",
    "        super(CovidPaperSimpleEnv, self).__init__()\n",
    "        self.DATA = pandas.read_csv(\"../data/rl_demo_data.csv\")\n",
    "        self.NUM_OF_PAPERS = self.DATA.shape[0]\n",
    "        self.REWARD_ILLEGAL_MOVE = np.asarray(-10., dtype=np.float32) # Illegal publish, i.e. republish\n",
    "        self.Counter = 0\n",
    "        \n",
    "        self._discount = np.asarray(discount, dtype=np.float32)\n",
    "        self._states = None\n",
    "        self._action_spec = array_spec.BoundedArraySpec((), np.int32, minimum=0, maximum=self.NUM_OF_PAPERS-1, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec((self.NUM_OF_PAPERS,), np.int32, minimum=0, maximum=1, name='observation')\n",
    "\n",
    "    def action_spec(self):\n",
    "        \"\"\"Assign the rows of data, that are papers, as possible actions\n",
    "        \"\"\"\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        \"\"\"The publishing status of papers as observation\n",
    "        \"\"\"\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._states = np.zeros((self.NUM_OF_PAPERS,), np.int32)\n",
    "        return TimeStep(StepType.FIRST, np.asarray(0.0, dtype=np.float32), self._discount, self._states)\n",
    "\n",
    "    def get_state(self) -> TimeStep:\n",
    "        # Returning an unmodifiable copy of the state.\n",
    "        return copy.deepcopy(self._current_time_step)\n",
    "\n",
    "    def set_state(self, time_step: TimeStep):\n",
    "        self._current_time_step = time_step\n",
    "        self._states = time_step.observation\n",
    "\n",
    "    def _step(self, action: np.ndarray):\n",
    "        \"\"\"Define step. Cases: last, illegal, middle ones(update if next state is last).\n",
    "        \"\"\"\n",
    "        \n",
    "        if self._current_time_step.is_last():\n",
    "            # print(\"Step: \" + str(self.Counter))\n",
    "            return self._reset()\n",
    "        \n",
    "        self.Counter = self.Counter + 1\n",
    "        \n",
    "        if self._states[action] != 0:\n",
    "            return TimeStep(StepType.LAST, self.REWARD_ILLEGAL_MOVE, self._discount, self._states)\n",
    "\n",
    "        self._states[action] = 1\n",
    "\n",
    "        is_final, reward = self._check_states(self._states)\n",
    "        \n",
    "        step_type = StepType.MID\n",
    "        if np.all(self._states == 0):\n",
    "            step_type = StepType.FIRST\n",
    "        elif is_final:\n",
    "            step_type = StepType.LAST\n",
    "\n",
    "        return TimeStep(step_type, reward, self._discount, self._states)\n",
    "\n",
    "    def _check_states(self, states: np.ndarray):\n",
    "        \"\"\"Check if the given states are final and calculate reward.\n",
    "            Args:\n",
    "              states: states of the board.\n",
    "\n",
    "            Returns:\n",
    "              A tuple of (is_final, reward) where is_final means whether the states\n",
    "              are final are not, and reward is the reward for stepping into the states\n",
    "        \"\"\"\n",
    "        # Reward: Summation of citations for all published papers\n",
    "        rewards = np.asarray(np.sum(self.DATA['Cit'].to_numpy().take(np.where(states == 1)[0])), dtype=np.float32)\n",
    "\n",
    "        if 0 in states:\n",
    "            return False, rewards\n",
    "        \n",
    "        return True, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init environment ...\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.environments import utils\n",
    "\n",
    "env = CovidPaperSimpleEnv()\n",
    "utils.validate_py_environment(env, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and eval use the same environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "The algorithm used to solve an RL problem is represented by an `Agent`. TF-Agents provides standard implementations of a variety of `Agents`, including:\n",
    "\n",
    "-   [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (used in this tutorial)\n",
    "-   [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
    "-   [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
    "-   [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n",
    "-   [PPO](https://arxiv.org/abs/1707.06347)\n",
    "-   [SAC](https://arxiv.org/abs/1801.01290).\n",
    "\n",
    "The DQN agent can be used in any environment which has a discrete action space.\n",
    "\n",
    "At the heart of a DQN Agent is a `QNetwork`, a neural network model that can learn to predict `QValues` (expected returns) for all actions, given an observation from the environment.\n",
    "\n",
    "Use `tf_agents.networks.q_network` to create a `QNetwork`, passing in the `observation_spec`, `action_spec`, and a tuple describing the number and size of the model's hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `tf_agents.agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
    "\n",
    "Agents contain two policies: \n",
    "\n",
    "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
    "-   `agent.collect_policy` — A second policy that is used for data collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
    "\n",
    "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
    "-   `state` — used for stateful (that is, RNN-based) policies\n",
    "-   `info` — auxiliary data, such as log probabilities of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Evaluation\n",
    "\n",
    "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
    "\n",
    "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this computation on the `random_policy` shows a baseline performance in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5469.2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
    "\n",
    "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=100)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 348), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.int32, action=tf.int32, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fb0a50b0e10>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent\n",
    "\n",
    "Two things must happen during the training loop:\n",
    "\n",
    "-   collect data from the environment\n",
    "-   use that data to train the agent's neural network(s)\n",
    "\n",
    "This example also periodicially evaluates the policy and prints the current score.\n",
    "\n",
    "The following will take ~5 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 19827.533203125\n",
      "step = 400: loss = 80905.7421875\n",
      "step = 600: loss = 173070.59375\n",
      "step = 800: loss = 568271.8125\n",
      "step = 1000: loss = 172214.765625\n",
      "step = 1000: Average Return = 16.0\n",
      "step = 1200: loss = 1455545.0\n",
      "step = 1400: loss = 2601164.5\n",
      "step = 1600: loss = 1691220.375\n",
      "step = 1800: loss = 2689059.5\n",
      "step = 2000: loss = 7292276.0\n",
      "step = 2000: Average Return = 16.0\n",
      "step = 2200: loss = 15636450.0\n",
      "step = 2400: loss = 10139023.0\n",
      "step = 2600: loss = 50049744.0\n",
      "step = 2800: loss = 1464943.5\n",
      "step = 3000: loss = 39674640.0\n",
      "step = 3000: Average Return = 16.0\n",
      "step = 3200: loss = 2368655.0\n",
      "step = 3400: loss = 63558060.0\n",
      "step = 3600: loss = 24347168.0\n",
      "step = 3800: loss = 72446336.0\n",
      "step = 4000: loss = 1613992.75\n",
      "step = 4000: Average Return = 16.0\n",
      "step = 4200: loss = 5826146.5\n",
      "step = 4400: loss = 75772368.0\n",
      "step = 4600: loss = 43258832.0\n",
      "step = 4800: loss = 3002481.5\n",
      "step = 5000: loss = 153190064.0\n",
      "step = 5000: Average Return = 16.0\n",
      "step = 5200: loss = 66679328.0\n",
      "step = 5400: loss = 8426392.0\n",
      "step = 5600: loss = 122399976.0\n",
      "step = 5800: loss = 4585294.0\n",
      "step = 6000: loss = 7096656.0\n",
      "step = 6000: Average Return = 16.0\n",
      "step = 6200: loss = 441831392.0\n",
      "step = 6400: loss = 20969380.0\n",
      "step = 6600: loss = 5456450.0\n",
      "step = 6800: loss = 5948538.5\n",
      "step = 7000: loss = 305986080.0\n",
      "step = 7000: Average Return = 16.0\n",
      "step = 7200: loss = 12355020.0\n",
      "step = 7400: loss = 19640314.0\n",
      "step = 7600: loss = 7994052.5\n",
      "step = 7800: loss = 288317120.0\n",
      "step = 8000: loss = 391211520.0\n",
      "step = 8000: Average Return = 16.0\n",
      "step = 8200: loss = 21755890.0\n",
      "step = 8400: loss = 30606312.0\n",
      "step = 8600: loss = 110210712.0\n",
      "step = 8800: loss = 8559770.0\n",
      "step = 9000: loss = 35941496.0\n",
      "step = 9000: Average Return = 16.0\n",
      "step = 9200: loss = 24476780.0\n",
      "step = 9400: loss = 44381040.0\n",
      "step = 9600: loss = 912250752.0\n",
      "step = 9800: loss = 9754919.0\n",
      "step = 10000: loss = 27651844.0\n",
      "step = 10000: Average Return = 16.0\n",
      "step = 10200: loss = 28755080.0\n",
      "step = 10400: loss = 205330912.0\n",
      "step = 10600: loss = 86795536.0\n",
      "step = 10800: loss = 511209952.0\n",
      "step = 11000: loss = 804754368.0\n",
      "step = 11000: Average Return = 16.0\n",
      "step = 11200: loss = 111677008.0\n",
      "step = 11400: loss = 70346712.0\n",
      "step = 11600: loss = 47337412.0\n",
      "step = 11800: loss = 30218222.0\n",
      "step = 12000: loss = 112360072.0\n",
      "step = 12000: Average Return = 16.0\n",
      "step = 12200: loss = 16330092.0\n",
      "step = 12400: loss = 75822016.0\n",
      "step = 12600: loss = 163930448.0\n",
      "step = 12800: loss = 22313952.0\n",
      "step = 13000: loss = 281502272.0\n",
      "step = 13000: Average Return = 16.0\n",
      "step = 13200: loss = 113572640.0\n",
      "step = 13400: loss = 175688048.0\n",
      "step = 13600: loss = 101382240.0\n",
      "step = 13800: loss = 101376864.0\n",
      "step = 14000: loss = 97385160.0\n",
      "step = 14000: Average Return = 16.0\n",
      "step = 14200: loss = 59276092.0\n",
      "step = 14400: loss = 175578288.0\n",
      "step = 14600: loss = 165946656.0\n",
      "step = 14800: loss = 126742752.0\n",
      "step = 15000: loss = 163172656.0\n",
      "step = 15000: Average Return = 16.0\n",
      "step = 15200: loss = 231235392.0\n",
      "step = 15400: loss = 88002432.0\n",
      "step = 15600: loss = 220506624.0\n",
      "step = 15800: loss = 97835536.0\n",
      "step = 16000: loss = 145573216.0\n",
      "step = 16000: Average Return = 16.0\n",
      "step = 16200: loss = 146194128.0\n",
      "step = 16400: loss = 1311479424.0\n",
      "step = 16600: loss = 213941920.0\n",
      "step = 16800: loss = 2248054016.0\n",
      "step = 17000: loss = 59050096.0\n",
      "step = 17000: Average Return = 16.0\n",
      "step = 17200: loss = 255946992.0\n",
      "step = 17400: loss = 359200768.0\n",
      "step = 17600: loss = 1703080832.0\n",
      "step = 17800: loss = 104359624.0\n",
      "step = 18000: loss = 6607588864.0\n",
      "step = 18000: Average Return = 16.0\n",
      "step = 18200: loss = 817911744.0\n",
      "step = 18400: loss = 483837312.0\n",
      "step = 18600: loss = 163721984.0\n",
      "step = 18800: loss = 466276704.0\n",
      "step = 19000: loss = 189668528.0\n",
      "step = 19000: Average Return = 16.0\n",
      "step = 19200: loss = 172238304.0\n",
      "step = 19400: loss = 1114775296.0\n",
      "step = 19600: loss = 183714080.0\n",
      "step = 19800: loss = 280688736.0\n",
      "step = 20000: loss = 656900416.0\n",
      "step = 20000: Average Return = 16.0\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "### Plots\n",
    "Use `matplotlib.pyplot` to chart how the policy improved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.95, 250.0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
